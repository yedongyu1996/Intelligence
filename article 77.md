 # 人工智能治理框架的情报事实解读 (2024年6月)

## 1. 研究对象
- **研究领域**: 人工智能 (AI) 治理, 负责任AI (Responsible AI)。
- **核心对象**: 全球范围内由不同机构发布的AI治理框架的原则、核心要素与实践情况。
- **数据来源或案例**:
    - **国际组织框架**: 经济合作与发展组织 (OECD)、欧盟委员会。
    - **企业框架**: Google、微软。
    - **政府与国防部门**: 美国国防部的AI伦理原则与战略。
    - **统计数据库**: AI事故数据库 (AIID)、AIAAIC事件与争议数据库、OECD负责任AI工具与指标目录。
    - **中国相关文件**: 《人工智能北京共识》、《新一代人工智能治理原则——发展负责任的人工智能》。

## 2. 研究方法
- **文献分析与比较研究**: 对比分析了由政府机构、国际组织和企业发布的多个代表性负责任AI框架 (如OECD, 欧盟, Google, 微软), 识别其内容的趋同性与差异性。
- **概念辨析**: 对“负责任AI”、“值得信赖的AI”和“符合道德的AI”等相关术语进行辨析, 明确了“负责任AI”作为更全面、包容的核心概念。
- **核心要素提取**: 从多个主流治理框架中归纳和提炼出四个共通的核心要素 (问责制、透明性、公平、可解释性)。
- **描述性统计分析**: 引用AIID数据库的统计数据, 分析AI事故造成的伤害类型、涉及行业及责任方分布；利用OECD目录数据, 分析负责任AI工具包和评价指标在AI生命周期及各项原则上的分布情况。

## 3. 研究出发点与创新性
- **背景与动机**: AI技术在带来巨大社会效益的同时, 也因其“黑箱”特性和强大的能力而蕴含巨大风险。特别是生成式AI的出现, 加剧了虚假信息等负面影响。国际社会 (包括AI开发者本身) 已普遍认识到管控AI风险的紧迫性, 亟需建立有效的治理框架。本文旨在通过对现有治理框架的解读, 为中国推进负责任AI提供借鉴。
- **创新点**:
    1. **时效性**: 研究背景涵盖了ChatGPT发布后的新形势, 对AI风险的理解和治理框架的分析更贴近当前的技术前沿。
    2. **要素提炼**: 超越了对单一原则的罗列, 首次从多个权威框架中抽象出“问责制、透明性、公平、可解释性”四个核心要素, 提供了更具概括性的分析视角。
    3. **实践导向**: 不仅关注理论原则, 还通过分析OECD的工具和指标目录以及美国国防部的实践路径, 将高层级的治理原则与具体的落地工具和实践探索联系起来, 强调了可操作性。
    4. **情报视角**: 借鉴情报学方法, 将治理框架作为“情报事实”进行刻画, 并特别关注了国防与情报领域的AI治理实践, 为中国提供了独特的国家安全层面的政策建议。

## 4. 详细研究内容
### 4.0 引言 (Introduction)
- AI技术是一把双刃剑, 在提升生产力的同时, 其如同“黑箱”的运行方式带来了潜在风险。
- 国际AI安全中心 (CAIS) 及OpenAI的CEO等行业领袖共同发声, 强调应将减轻AI可能导致的社会规模风险 (如灭绝风险) 作为全球性的优先事项, 与防范流行病、核战争同等重要。
- 为有效管控风险, 促进人机和谐, 有必要构建AI治理框架。本文将采用情报刻画的方法来研究这些框架。

### 4.1 人工智能的风险分析 (AI Risk Analysis)
- 生成式AI (GenAI) 因能创造新内容, 其风险比传统分析型AI更大, 易被用于制造和传播虚假信息, 造成严重的社会、政治和经济后果。
- 来源于AI事故数据库 (AIID) 的统计显示:
    - **主要伤害类型**: 对社会/政治制度的危害 (18.6%), 心理伤害 (17.6%), 危害身体健康/安全 (16.7%)。
    - **主要涉及行业**: 信息通信业 (28.0%), 运输仓储业 (14.0%), 艺术娱乐业 (14.0%)。
    - **主要责任方**: 均为大型科技公司, 如谷歌 (15.1%)、亚马逊 (5.6%)、特斯拉 (4.0%) 等, 表明这些公司在推动AI应用的同时, 对其危害的重视程度有待加强。
- 另一数据库AIAAIC的数据也表明, 自2012年以来, AI相关的事件和争议数量呈逐年上升趋势。

### 4.2 “负责任AI”术语概念的渊源 (Origin of the "Responsible AI" Concept)
- 目前对“负责任AI” (Responsible AI) 尚无统一官方定义, 其核心思想是对AI带来的力量负责。
- 在AI治理领域, 常用的三个概念是：
    - **值得信赖的AI (Trustworthy AI)**: 侧重于技术层面, 如减轻偏见、保证公平、透明和可解释。
    - **符合道德的AI (Ethical AI)**: 侧重于价值观, 但因不同群体的道德标准不一且可能冲突, 难以形成统一标准。
    - **负责任的AI (Responsible AI)**: 是一个更全面和包容的概念, 不仅涵盖了“值得信赖”和“符合道德”的内容, 更强调确保AI尊重并维护人权和社会价值, 因此成为使用最广泛的术语。

### 4.3 人工智能治理框架的核心要素 (Core Elements of AI Governance Frameworks)
- 全球各行业和组织发布的AI伦理原则虽表述各异, 但主题高度趋同。
- 本文选取了四个有代表性的负责任AI框架进行分析:
    - **经合组织 (OECD, 2019)**: 包含包容性增长、以人为本、透明度、健壮性、问责制等5项原则。
    - **欧盟委员会 (2020)**: 提出可信赖AI的7个关键要素, 包括人类监督、技术稳健性、隐私、透明度、多样性、社会福祉、问责制。
    - **Google (2022)**: 更新了其7项原则, 包括对社会有益、避免偏见、为安全而建、对人负责、融入隐私设计等。
    - **微软 (2022)**: 发布第二版标准, 包含问责制、透明性、公平性、安全可靠性、隐藏性 (应为隐私性)、包容性等6项。
- 通过对上述框架的综合分析, 论文提炼出四个共同的**核心要素**:
    - **问责制 (Accountability)**: AI的开发者、提供者和使用者需对其行为和结果承担相应责任。
    - **透明性 (Transparency)**: AI系统的设计、实现和应用过程应可见、可知、可理解。
    - **公平 (Fairness)**: AI系统应尊重和保护各方合法权益, 避免产生不公或歧视。
    - **可解释性 (Explainability)**: AI系统应能向利益相关者解释其决策的原因和逻辑。
- 这四大要素构成了负责任AI治理的基石。

### 4.4 人工智能治理的实践 (The Practice of AI Governance)
- 将原则付诸实践比制定原则更为重要。许多机构已开发工具包和量化指标来推动负责任AI的落地。
- **OECD的工具与指标目录**:
    - 截至2023年6月, 共收录577个工具包和101个评价指标。
    - **工具包分布**: 在AI生命周期的六个阶段中, “算法验证” (60个)、“模型构建” (55个) 和“规划设计” (53个) 的工具最多。
    - **指标分布**: 涉及“系统性能”的指标最多 (53个), 其次是“透明度和可解释性” (16个)。而涉及“公平性” (7个) 和“隐私与数据治理” (4个) 的量化指标严重不足, 表明这两方面是当前实践中的难点和痛点。
- **美国国防领域的实践**:
    - 美国国防部高度关注AI技术, 旨在利用AI维持军事优势。
    - 2020年通过了AI伦理5项原则: 负责任的 (Responsible)、公平的 (Equitable)、可追溯的 (Traceable)、可靠的 (Reliable)、可治理的 (Governable)。
    - 2022年发布了《负责任的人工智能战略与实施路径》, 指导AI在国防领域的合乎伦理的应用。

### 4.5 推进我国人工智能治理的建议 (Suggestions for Promoting AI Governance in China)
- 我国已发布《人工智能北京共识》等原则性文件, 但在实践落地方面与国际领先水平存在差距 (例如在OECD目录中, 来自中国的工具包仅5个, 而美国有90个)。
- **具体建议**:
    - **重视负责任AI原则的实践**: 政府应出台政策推动落地, 鼓励产学研联合攻关, 加强负责任AI的研究与实践。
    - **推进负责任AI在国防领域的实施**: 建议我国国防部成立专门机构, 类似美国国防部首席数字和人工智能办公室 (CDAO), 统筹国防领域的AI战略与应用, 确保国家安全。
    - **加强情报界在负责任AI中的研究**: 我国情报界应借鉴美国同行的做法, 制定情报领域的AI伦理原则和指南, 并发挥信息组织优势, 构建中国的AI事故事实数据库。
    - **把握负责任AI可持续发展的关键**: 围绕“问责制、透明性、公平性、可解释性”四个核心要素, 从法律框架、第三方监管、偏见消减、可解释性技术研发等方面系统性地推进我国AI治理。

## 5. 研究结论
- **主要发现**:
    - 全球人工智能治理框架在多样化的表述下, 共同指向了问责制、透明性、公平和可解释性这四个核心要素。
    - 负责任AI的实践是当前治理的重点和难点, 尤其是在公平性和隐私保护方面, 缺乏成熟的量化评估指标和工具。
    - 美国等国家已将负责任AI的实践深入到国防和情报等关键领域, 旨在确保其在未来竞争中的伦理与战略优势。
- **政策与实践意义**:
    - 中国需要从发布AI原则的阶段, 转向系统性地推动原则落地的实践阶段。
    - 美国国防部在AI治理上的战略布局为我国提供了重要参考, 即必须将负责任AI与国家安全紧密结合。
    - 情报界在AI治理中可以发挥独特作用, 通过建立事实数据库等方式, 为AI系统的安全开发提供数据支持和决策参考。
- **未来建议**:
    1. **强化实践落地**: 政府、高校和企业需协同合作, 加大力度研发负责任AI的落地工具和实践方案。
    2. **聚焦国防安全**: 建议在国防领域设立专门的AI治理与战略统筹机构, 确保AI技术在军事应用中的安全、可靠和可控。
    3. **发挥情报优势**: 推动情报界制定适用于自身业务的AI伦理指南, 并牵头建立国家级AI事故数据库。
    4. **夯实核心要素**: 通过完善法律法规、引入第三方认证、加强技术研发等手段, 全面加强问责制、透明性、公平性和可解释性在AI系统全生命周期中的实现。