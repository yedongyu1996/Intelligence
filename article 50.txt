 # 大语言模型赋能网络威胁情报分析：场景、风险和发展路径 (2025年4月)

研究对象

研究领域: 网络威胁情报 (Cyber Threat Intelligence, CTI) 分析。
核心对象: 大语言模型 (Large Language Models, LLMs)。
研究维度: 探讨 LLMs 在 CTI 分析中的应用场景、风险挑战与未来发展路径。
案例来源:
    谷歌 BERT 模型及其衍生品 SecurityBERT。
    OpenAI GPT-4 模型及其在网络事件分析中的应用。

研究方法

案例分析法: 通过剖析两个具体的大语言模型（BERT 和 GPT-4）在网络安全领域的实际应用案例，来具象化阐述其在威胁情报分析中的效用、挑战和发展方向。
技术性综述: 从技术视角出发，系统性地梳理和论证大语言模型赋能网络威胁情报分析的内在机理、主要应用方式、潜在的技术与非技术风险，并在此基础上提出相应的解决路径。

研究出发点与创新性

背景与动机: 人工智能技术已成为辅助国家安全决策的关键模式。在网络安全领域，威胁情报分析虽流程基本确定，但缺乏统一标准且依赖人工。以大语言模型为代表的生成式 AI 为提升情报分析的科学性与效率提供了新机遇，但现有研究多集中于具体技术开发，缺乏对应用场景、风险与发展路径的系统性阐述。
创新点:
    系统性地归纳了 LLMs 在 CTI 分析中的三大核心应用场景：网络安全威胁检测、大规模文本分析和情报报告自动生成。
    深入剖析了 LLMs 应用于 CTI 分析时面临的三重主要风险：数据稀缺（由商业垄断导致）、数据质量问题（源于虚假信息与技术偏见）和语言障碍（源于英语中心主义）。
    提出了一个结合机制建设与技术革新的综合性发展路径，涵盖构建公私合作伙伴关系（PPP）、应用知识图谱与创新编码算法，以及开发多语种大语言模型。

详细研究内容

4.1 大语言模型在网络威胁情报分析中的应用场景

网络安全威胁检测:
    LLMs 能够通过分析自然语言和源代码来检测安全漏洞并生成修复补丁。
    模型可根据文本报告和行为描述对恶意软件进行分类。
    通过分析邮件内容和语言模式，LLMs 能有效识别网络钓鱼攻击。
    模型能够通过模拟真实攻击场景来测试和加固现有安全系统的稳健性。
    最终目标是利用 LLMs 主动识别并消除潜在威胁，缩短威胁识别时间，制定更快速、精准的缓解策略。
大规模文本分析:
    LLMs 的核心优势在于其“零样本分析能力”(zero-shot capability)，即无需为特定任务进行专门训练就能执行文本分析。
    一个模型可以同时处理主题识别、情感分析、攻击性语言检测等多种复杂任务，简化了传统上需要为每项任务单独建模的流程。
    以 GPT-4 为例，其强大的自然语言处理能力能够高精度地解读网络安全文本的细微差别和内在联系，处理包含技术术语和行业术语的文本。
情报报告自动生成:
    LLMs 能够对大规模网络威胁情报进行有效的存储和分类。
    模型可以将来自多个来源的情报进行综合解析，生成关于特定威胁或事件的多源信息报告。
    通过汇总特定时间段内的情报，LLMs 可以构建网络安全威胁的时间线，帮助分析人员深入理解威胁的演变过程。
    文章以意大利帕多瓦大学开发的 AGIR 工具为例，该工具基于 LLM，利用结构化威胁信息（STIX）自动生成概述、主题、时间线和漏洞四种类型的报告，显著提升了报告的流畅性与撰写效率。

4.2 大语言模型在网络威胁情报分析中面临的风险挑战

数据稀缺:
    此处的“稀缺”并非指网络数据总量的匮乏，而是指数据的“病态化稀缺”，即数据所有权的私人化和商业化。
    全球数据主要由私人科技公司掌握，这些公司出于商业利益和用户隐私保护，会限制外部对数据的访问。
    国家情报机构在进行威胁分析时，所需的数据集（包括私人、商业数据）难以直接获取，必须通过商业途径购买，但科技公司可能因企业伦理、公众形象等原因不愿与政府（特别是军方）合作。
    这种数据垄断现象使得国家主导的情报工作在技术和法律层面受到双重制约。
数据质量难题:
    虚假网络信息: 蓄意制造和传播的虚假信息（如通过社会工程学、微观定位等方式）会污染 LLM 的训练语料库，导致模型对网络威胁度的判断出现偏差，可能引发过度激进的安全应对策略。
    标记化偏见和信息完整性降低: 在将文本转化为模型可读的“令牌”（token）过程中，不同的编码方案（如 BPE）会产生采样偏差或信息丢失，降低了数据的完整性和可靠性。例如，字符级令牌化可能丧失词级含义。这会导致情报输出不准确，影响决策科学性。
语言障碍:
    LLMs 普遍存在“以英语为母语”的偏见，因为互联网上绝大多数可用于训练的文本是英语，而多数分析工具（如情感词典）也以英文为基础开发。
    这种语言单一性导致威胁情报来源片面，模型在处理非英语文本或俚语、讽刺等复杂语用时准确性下降。
    跨语言翻译的质量问题也限制了模型的应用范围，英语的主导性在模型训练中被放大，形成语言失衡导致的算法偏见。

4.3 案例研究：基于BERT和GPT-4的网络威胁情报分析实践

SecurityBERT 模型:
    由阿联酋技术创新研究所（TII）于2024年8月推出，是一个基于 BERT 架构的轻量级模型，专为物联网（IoT）设备上的网络威胁检测而设计。
    该模型采用了一种新颖的隐私保护编码技术（PPFLE）和字节级字节对编码（BBPE）标记器，有效处理网络流量数据，同时降低了计算资源需求。
    模型架构为15层，大小仅为16.7MB，适宜在资源受限设备上部署。
    在使用 Edge-IIoTset 数据集进行的测试中，该模型能在0.3秒内识别14种不同类型的网络攻击，总体准确率达到98.2%，性能优于多种传统机器学习及混合语言模型。
基于网络事件分析的GPT-4开源情报生成模型:
    由莫纳什大学学者于2024年4月提出，该方法利用 GPT-4 分析美国战略与国际研究中心（CSIS）的重大网络事件开源数据（2016-2023年间的214起事件）。
    其分析流程包括定义提取参数、加载文本、提取信息、异常检测、分解分析、问答查询和生成智能叙述等9个步骤。
    模型从事件语料库中提取了关于参与者类型、目标、攻击源、目的地、级别、类型和时间线等七个维度的信息，信息提取的精确度达到96%，召回率达到98%。
    研究结果表明，该方法在理解威胁模式、识别异常事件和改进网络安全决策方面表现出色。

4.4 基于机制完善和技术革新的发展路径

构建网络威胁情报分析的PPP机制:
    为应对数据垄断和网络安全问题的复杂性，建议构建政府与私营科技公司间的公私合作伙伴关系（Public-Private Partnership）。
    该机制旨在通过数据与技术共享、风险共担和协同创新，将科技公司的数据和技术优势与政府的协调职能相结合。
    通过建立基于共同安全认知和市场基础的合作，有望打破数据“病态化”稀缺的困境，降低政府压力，整体提升国家网络安全治理能力。
应用与创新网络安全知识图谱(CKG)和编码算法:
    应对虚假信息: 采用网络安全知识图谱（Cybersecurity Knowledge Graphs, CKG）技术。CKG 通过图数据模型整合多源信息，可以有效检测和识别虚假的威胁情报，提升情报数据的质量。
    应对标记化偏见: 对现有的数据令牌化编码方案进行创新和调整。文章提到了“最大前缀校正算法”(Maximum Prefix Correction Algorithm)，该算法使用概率方法来消除标记化过程中的采样偏差，从而提升模型分析结果的准确性。
开发多语种大语言模型:
    这是解决语言障碍最直接的方法。多语种 LLM 能够同时处理和分析不同语言的网络威胁情报，提升分析的全面性。
    开发路径包括：
        参数调整: 在预训练、微调等不同阶段调整模型参数以实现跨语言对齐。
        参数冻结: 在不调整参数的情况下，通过提示策略（如直接提示、代码转换提示）来执行跨语言任务。
    作者同时指出，开发多语种模型时必须解决模型幻觉、安全性、公平性等固有挑战，以确保其在情报分析中的可靠性。

研究结论

主要发现:
    大语言模型（LLMs）能够显著提升网络威胁情报分析的整体水平，特别是在自动化进行网络威胁检测、处理大规模文本信息以及生成情报报告方面展现出巨大潜力。
    LLMs 在网络威胁情报领域的应用也面临着严峻的挑战，主要包括：因数据商业化垄断导致的“病态化”数据稀缺问题；由虚假网络信息和数据标记化偏见共同引发的数据质量下降问题；以及因训练数据和算法工具以英语为中心而造成的语言障碍与模型偏见问题。
实践意义与发展路径:
    机制层面: 建议通过构建政府与私营技术企业间的公私合作伙伴关系（PPP）机制，来打破数据孤岛，实现数据与技术资源的共享，协同应对网络威胁。
    技术层面:
        应积极应用网络安全知识图谱（CKG）等技术来识别和过滤虚假威胁情报。
        需要持续创新数据编码算法，以缓解和消除数据预处理阶段产生的标记化偏见。
        大力开发多语种大语言模型是克服语言障碍、确保情报分析全面性和准确性的关键。
未来工作: 在推进多语种大语言模型发展的过程中，必须重点关注并解决模型幻觉、安全性、公平性以及语言扩展性等难题，以保障其在关键安全领域的可靠应用。