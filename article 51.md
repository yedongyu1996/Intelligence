 # 大语言模型的应急情报生成能力测评基准 (2025)

## 1. 研究对象

-   **研究领域**: 应急情报学 (Crisis Informatics), 大语言模型 (LLMs) 评估.
-   **核心对象**: 具备中文处理能力的大语言模型 (LLMs) 在生成应急情报方面的能力.
-   **测评模型列表**:
    -   **国外模型**:
        -   Claude 3.5 Sonnet
        -   GPT-4.0
        -   Llama-3.1-405B
        -   Gemini-1.5-Pro
    -   **国内模型**:
        -   文心大模型 4.0 Turbo (ERNIE 4.0 Turbo)
        -   讯飞星火 V4.0 (iFlytek Spark V4.0)
        -   GLM-130B
        -   通义千问-72B (Qwen-72B)
-   **数据来源**: 研究团队构建的名为 `CIEval` 的综合评估数据集. 该数据集覆盖四大类共计 26 种具体突发事件场景, 包括自然灾害、事故灾难、公共卫生事件和社会安全事件.

## 2. 研究方法

-   **基准构建 (Benchmark Construction)**
    -   **用途**: 构建一个专门用于评估大模型应急情报生成能力的综合性数据集 `CIEval`.
    -   **设计**: 采用“人机结合、循环迭代”的思路. 首先从国家应急管理法律法规中划分出四大类突发事件, 再细分为 26 个具体场景. 随后, 利用 GPT-4.0 从真实事件语料中提取信息并生成问答对 (Prompt), 最后由人工审核和修正以确保高质量.

-   **多维度评估指标体系**
    -   **用途**: 从内容、表达、可行性和效用四个层面科学、全面地衡量模型生成情报的质量.
    -   **具体指标**:
        -   **内容质量 ($C_1$)**: 包含准确性、完整性、时效性.
        -   **表达质量 ($C_2$)**: 包含逻辑性、简洁性.
        -   **可行程度 ($C_3$)**: 衡量所提建议的可操作性.
        -   **效用质量 ($C_4$)**: 包含实用指导价值和决策支撑价值.

-   **组合式评分方法**
    -   **用途**: 结合人工与机器评分的优点, 提升评估的客观性和效率.
    -   **设计**:
        -   **人工评分**: 由领域专家根据评估指标体系进行主观打分, 能够评估深层语义和实用性.
        -   **机器评分**: 使用 GPT-4 作为辅助评估工具, 对客观性、完整性等指标进行量化打分, 验证结果表明其与人工评分有显著正相关性.

-   **多准则群决策方法 (TODIM)**
    -   **用途**: 对多源、异构的评估数据进行融合计算, 得出各模型的最终排序.
    -   **前提条件**: 该方法源于前景理论, 能够处理评估过程中专家的模糊和不确定性判断.
    -   **关键参数**: 采用三角直觉模糊数来表示专家的语言评价等级 (如“完美高”、“中等”等), 并通过计算优势度函数 $\delta(A_p, A_q)$ 和全局优势度 $\zeta_p$ 来确定模型的综合得分.

## 3. 研究出发点与创新性

-   **背景与动机**:
    -   大语言模型 (LLMs) 在自然语言处理方面展现出强大能力, 已开始在各领域应用.
    -   在应急管理领域, 如何利用 LLMs 快速、准确地生成情报以辅助决策, 成为一个重要课题.
    -   现有的 LLM 评估基准大多是通用性的 (如 GLUE, CLUE), 或面向其他特定领域, 缺乏一个专门针对应急情报生成任务的、科学且全面的评估框架.

-   **创新点**:
    1.  **构建首个应急情报生成测评基准 (CIEval)**: 针对性地设计并构建了一个覆盖 26 类突发事件场景的综合性评估数据集, 填补了该领域的空白.
    2.  **提出多维度的能力评估体系**: 不仅关注生成内容的准确性, 还综合考量了表达质量、建议可行性及决策效用, 使评估结果更贴近实战需求.
    3.  **采用人机结合与 TODIM 的综合评估方法**: 结合了专家经验与机器效率, 并运用多准则决策模型对评估结果进行科学加权与排序, 增强了评估的客观性和可信度.

## 4. 详细研究内容

### 4.1 引言 (Introduction)

-   该部分首先阐述了大语言模型 (LLMs) 在处理和生成信息方面的革命性影响, 及其在应急管理等关键领域的应用潜力.
-   接着, 作者指出当前对 LLMs 的评估主要集中在通用语言能力上, 现有的评估基准 (如 GLUE, SuperGLUE, C-Eval) 无法有效衡量模型在特定、高风险场景 (如应急响应) 中的情报生成质量.
-   因此, 存在一个明确的需求, 即建立一个科学的基准来专门评估 LLMs 在应急情报生成任务上的表现, 这项工作对于筛选和优化用于应急决策支持的AI工具至关重要.

### 4.2 研究设计 (Research Design)

-   **总体研究框架**: 研究分为三个阶段: 数据集生成、待评估模型选择、实施评估.
    1.  **数据集生成**: 从四大突发事件类别（自然灾害、事故灾难、公共卫生、社会安全）出发, 使用 GPT-4.0 从真实事件文档中生成指令和提示, 经人工审核后形成 `CIEval` 数据集.
    2.  **模型选择**: 选取了 8 个国内外主流且具备中文处理能力的 LLMs 作为评估对象.
    3.  **评估实施**: 采用人工评分和机器评分相结合的方法, 依据内容质量、表达质量、可行程度和效用质量四个一级指标对模型生成的内容进行打分, 并用 TODIM 方法整合出最终结果.
-   **CIEval 数据集构建**:
    -   **主题覆盖**: 包含了地震、台风、交通事故、网络攻击等 26 种具体场景.
    -   **生成流程**: 展示了一个具体案例, 如“2024年7月湖南衡阳山洪灾害”, 从中提取关键信息, 并自动生成多个角度的问题 (如“如何加强民宿风险防范?”), 形成最终的测试提示 (Prompt).
-   **评估指标与方法**:
    -   **评估指标体系**: 详细定义了四个一级指标和七个二级指标, 例如内容质量 ($C_1$) 分解为准确性 ($C_{11}$)、完整性 ($C_{12}$) 和时效性 ($C_{13}$).
    -   **评估方法**:
        -   人工评分采用语言术语集 (如“完美高(PH)”,“中(M)”) 进行评价.
        -   这些语言术语被转换为三角直觉模糊数, 例如 `PH` 对应 `((4, 5, 5); 0.8, 0.1)`.
        -   利用 TODIM 方法计算每个模型相对于其他模型的优势度, 最终归一化得到全局排序.

### 4.3 分析与讨论 (Analysis and Discussion)

-   **评分方法可行性**:
    -   通过对人工评分和机器评分 (GPT-4) 结果进行非参数检验 (Kendall 和 Spearman 相关性分析), 发现两者存在显著的正相关关系 ($\rho < 0.05$). 这证明了使用机器评分作为辅助评估手段是可行的, 可以提高评估效率.
-   **模型总体能力评估**:
    -   **综合得分**: Claude 3.5 Sonnet 以 0.95 的高分位居第一, 表现远超其他模型. GPT-4.0 以 0.71 分排第二.
    -   **国内外对比**: 国外模型平均得分为 0.56, 优于国内模型的平均分 0.45. 国内模型中, 讯飞星火 V4.0 (0.64) 和文心大模型 4.0 Turbo (0.45) 表现领先.
    -   **多维度表现**: 在完整性、合理性等多个具体指标上, Claude 3.5 Sonnet 和 GPT-4.0 均表现出色.
-   **分场景能力评估**:
    -   **四大类别**:
        -   **自然灾害和事故灾难**: Claude 3.5 Sonnet 在这两类复杂多变的场景中优势最为明显.
        -   **公共卫生和社会安全**: GPT-4.0 在这两类事件中表现相对更强.
    -   **具体场景 (热力图分析)**:
        -   Claude 3.5 Sonnet 在地震、台风、矿难等多个场景中得分接近满分, 显示出强大的综合分析和信息生成能力.
        -   讯飞星火 V4.0 在地震灾害、交通事故等场景中表现突出, 展现了其在特定领域的优化效果.
        -   文心大模型 4.0 Turbo 在处理涉外突发事件和经济安全事件方面有较好表现.
-   **讨论**:
    -   研究认为 Claude 3.5 Sonnet 的领先得益于其强大的上下文理解和信息整合能力.
    -   GPT-4.0 逻辑性强, 但有时生成内容过于通用.
    -   国内模型虽总体有差距, 但在结合中国国情和特定数据方面具备潜力. Llama-3.1 和 GLM-130B 等模型在此次测评中表现不佳, 有较大提升空间.

## 5. 研究结论

-   **主要发现**:
    1.  不同大语言模型在应急情报生成能力上存在显著差异. Anthropic 公司的 Claude 3.5 Sonnet 综合表现最佳, 尤其擅长处理信息复杂多变的自然灾害和事故灾难.
    2.  国外顶尖模型整体上优于国内模型, 但国内模型 (如讯飞星火、文心一言) 在部分特定场景下展现出独特的优势和竞争力.
    3.  模型的性能不仅体现在综合得分上, 还体现在不同危机类型和评估维度上的差异化表现, 不存在“一招鲜”的通用最优模型.

-   **实践意义与建议**:
    -   应急管理部门在选择和应用 LLMs 时, 应根据具体的突发事件类型和任务需求进行针对性选择. 例如, 在应对台风、地震等自然灾害时, 优先考虑使用 Claude 3.5 Sonnet.
    -   本研究构建的 `CIEval` 基准和评估框架可作为行业标准, 用于持续追踪和评测各类 LLMs 在应急领域的应用潜力.

-   **未来工作**:
    1.  **扩展评估维度**: 在现有基础上增加对生成内容的情感倾向、舆情引导潜力等维度的评估.
    2.  **丰富测试模型**: 纳入更多国内外厂商的大语言模型, 使评估更具广度.
    3.  **深化研究方向**: 未来可以探索如何将表现优异的通用大模型通过领域知识进行微调, 构建专门服务于应急管理决策的垂直领域大模型.