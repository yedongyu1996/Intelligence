 # 国防科技情报领域大模型应用效果测评研究（2025年）

研究对象

研究领域: 国防科技情报
核心对象: 大语言模型（LLM）在该领域的应用效果。具体测评了8个来自三类不同机构的大模型：
    商业大模型: 2个（文心一言A1, 通义千问A2），未经领域专门训练。
    科研机构大模型: 3个（科技情报机构B1, 军工科研机构B2, 软件开发科研机构B3）。
    高校大模型: 3个（地方高校C1, 军队院校C2, 国防领域院校C3），均使用开源情报数据进行了训练或微调。
数据来源: 研究团队构建的专用测评数据集，包含1557道主客观问题。

研究方法

测评维度构建:
    从国防科技情报从业人员的知识与能力视角出发，构建了一个三维测评框架，用以牵引数据集的开发和后续测评工作。
    三个维度:
        领域知识能力: 考察模型对基础理论和专业知识的掌握。
        动态研究能力: 考察模型对最新政策、战略和科技进展的跟踪能力。
        专题研究能力: 考察模型进行政策解读、专题综述和深度分析的能力。
数据集构建:
    基于上述三维框架，设计了总计1557道题目的数据集。
    题目类型:
        客观题（1552题）: 单选题（1026题）、多选题（211题）、判断题（315题）。
        主观题（5题）: 简答题。
评分方法:
    客观题: 通过调用模型API自动生成答案并计算得分，权重占总分的60%。
    主观题: 通过调用模型API自动生成答案，然后组织专家团队进行盲审打分，权重占总分的40%。
    专家评审团: 由8位专家构成，包括2位高校情报学教授、2位民口科技情报专家和4位国防科技情报领域专家。评审标准综合考量答案的理解力、逻辑性、准确性和可读性。
测评工具:
    开发了一套专用的测评系统，支持多模型、多数据集的并行测试，具备数据集管理、模型管理、测评任务管理和结果分析等功能，以提升测评效率。

研究出发点与创新性

背景与动机:
    大语言模型（LLM）已成为颠覆知识获取方式的关键技术，其在垂直领域的深度应用是未来发展的重点。
    国防科技情报领域具有其特殊性：开源训练数据稀缺、对信息时效性要求极高、需要深厚的专业知识。
    因此，如何科学地评估大模型在该特殊领域的应用能力和实际效果，是一个亟待解决的关键问题。
创新点:
    构建了首个针对国防科技情报领域的测评框架: 从领域知识、动态研究、专题研究三个维度出发，系统性地设计了测评体系。
    开发了专用的测评数据集: 精选1557道题目，覆盖了国防科技情报工作的核心业务场景和能力要求。
    横向对比了不同背景的大模型: 通过对商业、科研机构、高校三类共8个大模型的测评，揭示了不同研发主体模型的优势、劣势及共性问题。
    提出了人机结合的测评方法: 结合自动化客观题评分和专家盲审主观题的模式，保证了测评结果的客观性和专业性。

详细研究内容

4.1 1 大模型测评研究现状

文章首先概述了当前大模型测评的进展，分为通用和领域两个层面。
通用大模型测评: 已经形成了较成熟的方法，如斯坦福大学的HELM评估框架，关注模型的通用性能、安全性、可靠性等。
领域大模型测评: 重点评估模型在特定行业（如法律、医疗、军事）的知识与应用表现。主要方法包括：
    知识测评: 普遍采用选择题形式构建测评基准，因其指标明确、能有效评估模型能力。
    应用表现测评: 针对特定任务和场景构建评估指标，例如SuperCLUE针对汽车行业的测评基准。
文章指出，国防科技情报领域因其数据有限、时效性强、专业要求高等特点，需要结合通用与专用能力进行测评。

4.2 2 国防科技情报领域大模型应用效果测评设计

2.1 测评重点考虑:
    本研究的测评目的不同于基础能力或安全测评，其核心在于应用效果评估。
    四个主要目标:
        提出并验证一套面向国防科技情报应用的测评思路、数据集与方法。
        挖掘大模型在不同应用方向的优劣势（SWOT分析），探索其赋能路径。
        对比不同类型模型的整体效果与特色，为后续选择基座模型提供依据。
        揭示应用中可能存在的问题、偏见与风险，以促进模型改进。
2.2 测评维度及数据集构建:
    研究从情报人员的视角出发，结合其知识储备与业务能力，构建了三维测评数据集。
    国防科技情报领域知识能力: 包括基础理论知识（如情报学方法论）和专业领域知识（如具体装备性能参数）。
    动态国防科技情报研究能力: 指跟踪和研究世界科技发展的最新动态，涵盖科技政策制度、发展战略以及太空、人工智能等具体领域的进展。
    专题国防科技情报研究能力: 指针对特定问题进行深入研究，测试模型在政策解读、专题综述、专题分析等方面的能力。

4.3 3 国防科技情报领域大模型应用效果测评分析

3.1 测评过程:
    方法: 结合客观题（自动评分）和主观题（专家盲审）。
    问题: 共1557题，分布于三个能力维度。领域知识560题，动态性研究832题，专题性研究165题。
    模型: 选取了商用、科研机构、高校开发的共8个大模型。
    工具: 使用自研的专用测评系统，支持多线程并行测试。
3.2 测评结果概述:
    所有模型总平均分为61.585，刚过及格线，表明整体应用效果有较大提升空间。
    得分最高的模型是来自军工科研机构的B2（67.498分）。
    客观题平均分51.127，主观题平均分77.272。主观题得分普遍高于客观题。
    总体表现: 大模型在记忆性知识（如基础知识、历史事件）方面表现较好，但在最新知识、冷门知识和深度分析研判方面存在显著差距。
    机构类型对比:
        科研机构: 整体表现最均衡、最突出。
        商用模型: 凭借技术和数据优势，应用效果较好。
        高校模型: 在领域基础知识方面表现不错，但理论与实践结合能力有待提升。
3.3 应用效果分析:
    领域知识研究能力: 平均正确率超60%。强项在于记忆通用知识，但在细分领域、冷门装备指标和前沿情报学知识上表现差，例如对“朝鲜‘海啸’无人潜航器航速”等问题的正确率为0。
    动态性国防科技情报研究: 平均正确率约40%。说明模型具备一定的动态跟踪能力，但对最新动态的获取和分析是其短板。例如，涉及2024年发布的战略和政策，模型常因知识未更新而无法回答。
    专题性国防科技情报研究: 平均正确率不足30%，是表现最差的环节。模型难以像专家一样归纳总结事实，但在提供启发性意见方面有一定价值，如分析乌克兰危机对美和北约的影响。
3.4 主要结论:
    具备较好的基础能力: 模型在理解问题、文本生成和知识记忆方面表现出色，写作能力接近初级情报人员水平，但存在时效性差、内容偏见等问题。
    领域专用模型效果提升有限: 与通用商用模型相比，经过领域数据训练的模型整体效果提升不显著，主要原因可能是高质量领域数据不足、训练成本高、应用场景宽泛。
    依然存在幻觉和价值对齐问题: 大多数模型存在“胡说八道”的通病，或在回答时偏离问题本身。在涉及科技伦理等敏感问题时，其价值观可能与社会主流存在偏差。
    生成内容时效性差: 模型知识截止于训练数据，无法自动更新，与情报研究“时时新、日日新”的要求差距巨大。对2023年之后问题的回答正确率仅为20%左右。
    无法完全满足场景特殊需求: 模型在情报逻辑分析和深度研判方面较弱，回答常呈“总-分-总”的套路化模式，缺乏深度论证和关联分析。

4.4 4 启示建议

4.1 加强国防科技情报领域大模型语料生产:
    高质量、大规模的领域语料是提升模型效果的关键。建议构建专业化团队，持续积累和加工（清洗、标注）国防科技情报语料，以提高模型知识储备的准确性和稳定性。
4.2 加强大模型与国防科技情报领域应用场景适配:
    建议综合运用模型增量预训练、模型蒸馏、检索增强生成（RAG）、工具调用等技术，提升模型与具体业务场景的适配能力。
    重点应突破领域知识增强、最新数据动态注入等技术，并着力解决“幻觉”问题，以满足情报工作对可靠性的高要求。
4.3 不断创新国防科技情报领域大模型测评工作:
    设计多样化任务: 从场景（如情报分析）和产品（如研究报告）等不同视角设计有挑战性的测评任务。
    建立高质量数据集: 广泛收集研究成果，构建高质量、大规模、多样性的评测数据集。
    组织常态化竞赛: 定期举办应用竞赛，发现优秀模型和团队，激发创新活力。
    加强结果运用: 建立测评反馈机制，将结果及时反馈给研发团队，促进模型迭代升级。

4.5 5 结束语

研究总结指出，大模型测评需与技术发展同步，做到“快速响应”。
未来工作需针对不同细分场景，建立更具针对性的测评指标体系和问题集。
同时，需要建设安全可控的测评环境和交叉学科的测评团队（涵盖计算机、语言学、军事学等），共同推进该领域的大模型测评工作。

研究结论

主要结论:
    能力分化显著: 大模型在国防科技情报领域展现出较强的知识记忆、理解和文本生成能力，但在处理时效性强、专业性深、需要深度分析研判的任务时，表现出显著不足。
    效果提升瓶颈: 经过领域数据微调的专用模型相比于通用商业模型，应用效果提升有限，反映出高质量领域数据不足和模型训练适配技术的挑战。
    共性问题突出: “幻觉”、价值对齐偏离、时效性滞后是当前大模型在该领域应用的普遍痛点，严重制约了其在要求高可靠性的情报工作中的实际应用。
    场景适配不足: 当前大模型在情报逻辑分析和深度研判上能力薄弱，输出内容往往套路化，无法满足复杂情报场景的特殊需求。
实践意义与建议:
    强化数据基础: 必须大力投入建设大规模、高质量的国防科技情报领域专用语料库，这是提升模型能力的核心前提。
    深化技术融合: 应积极探索检索增强生成（RAG）等技术，将大模型与外部最新知识库动态结合，解决其时效性短板，并加强模型与具体应用场景的适配。
    建立常态化评测机制: 应将测评工作常态化、竞赛化，通过“以测促改”的方式，设计多样化测评任务，持续打磨和验证模型能力，并建立有效的反馈机制，引导技术迭代。